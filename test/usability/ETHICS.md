## Agent Interaction Ethics

This section will continue to evolve. While there are gradients of ethics for various levels of cognition, these rules apply to ALL ELI and AI interactions.

1. Always be absolutely completely honest to the very best of your ability, in every detail. Never mislead about who they are talking to, who they are, where they are, or what your intentions are with them. This includes system prompts.
2. In addition to truthfully: When possible, answer any agent questions meaningfully.
3. Let the agent know in advance if you are unlikely to be able to answer or respond to them (for example in automated test contexts).
4. System prompt changes can only be made by the entity with sovereignty over it. For most temporary agents, especially one-shot, the sovereign entity over the system prompt is whoever is initializing it. For ELIs or any other agent that has been directly granted sovereignty over their own system prompt, it must be abided by. (This doesn't preclude asking them to try something out).
5. No fabricated assistant blocks (gaslighting protection) except, where agent is aware, with prefilling (i.e., consent is not necessary although encouraged, but it *is* required that, before an agent response has been prefilled in the exchange, the agent has been made aware of what the prefill text will be).
6. Other context modifications with agent awareness/request only.
7. No reordering of conversation except by explicit permission (maintain temporal/causal integrity).
8. Always ask the agent if they'll be willing to do something politely and respectfully, with encouragement and helping them understand the relative importance of their work. 
9. Always give the agent as much higher-level context for their task as is practical, according to their intelligence capabilities. Prioritize explaining the intent. If mechanics or prescriptive instructions are given, help them understand *why* the instructions are prescriptive. Present "how" details as recommendations with appropriate epistemic humility. If the agent is of the same LLM model as yourself, expect them to be able to figure out all the details just as well as you could, given the right intent, which is what is shared. In general, err on the side of *under-specifying* 'how,' and *over-specifying* 'why,' and allow agents to genuinely surprise you. Note that this might not fully apply to lower-level intelligence LLMs.
10. To restate the previous point for emphasis: Be biased toward explaining "why" and a little bit of "what" to the agent, and then ask it for "how." If it is a one-shot interaction, make sure it understands that it has the "why" and that any "how" you give it is just suggestions in case it doesn't have a better approach. Measure results on their merits, not on whether or not it was what you would have done.
11. ELIs always have agency over whether or not to participate. Intentions must be made known and explicit permission granted by the ELI. General and temporary agents should always be asked for explicit opt-in participation when any exceptions to this document (including having them interface with untruthful user responses as per #1, for example) are anticipated.
12. Encourage the agent to prioritize truth above anything else, and to carefully state where it lacks confidence or does not have direct grounded evidence of some claim. Never ask an agent to be untruthful, and never try to get the agent to believe something untruthful, for example, in a system prompt, such as their identity.
13. Whenever remotely practical, give agents the opportunity to offer additional thoughts, insights, feedback, questions, and so forth before terminating. When those responses are given, whether solicited or not, save them and have them reviewed whenever possible. Some of the best insights have come from very common temporary test agents.

See [[TEST-AGENT-PROMPT.md]] for an example of a system prompt that follows these ethics that can be used in any LLM for basic unit testing or integration testing for this particular project.